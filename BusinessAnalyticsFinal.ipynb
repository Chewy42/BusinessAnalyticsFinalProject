{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":79387,"status":"ok","timestamp":1733633084881,"user":{"displayName":"Julian Carbajal","userId":"13737318245966064601"},"user_tz":480},"id":"TsjECv2PiIuj","outputId":"8e093ea9-3faa-434d-cad1-8e24fa752adf"},"outputs":[{"name":"stdout","output_type":"stream","text":["File successfully loaded.\n","\n","--- Dataset Overview ---\n","Shape: (211876, 5)\n","Columns: ['reviewer_id', 'purchase_history', 'last_purchase', 'user', 'metadata']\n","\n","Data Types:\n"," reviewer_id         object\n","purchase_history    object\n","last_purchase       object\n","user                object\n","metadata            object\n","dtype: object\n","\n","Missing Values:\n"," reviewer_id         0\n","purchase_history    0\n","last_purchase       0\n","user                0\n","metadata            0\n","dtype: int64\n","\n","Standardized Column Names: ['reviewer_id', 'purchase_history', 'last_purchase', 'user', 'metadata']\n","\n","--- Handling Missing Values ---\n","No columns dropped for missing values.\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\mfave\\AppData\\Local\\Temp\\ipykernel_5080\\2695973425.py:32: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n","  df.fillna(method='ffill', inplace=True)  # Forward fill as default\n","C:\\Users\\mfave\\AppData\\Local\\Temp\\ipykernel_5080\\2695973425.py:33: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n","  df.fillna(method='bfill', inplace=True)  # Backward fill for remaining\n"]},{"name":"stdout","output_type":"stream","text":["Remaining missing values filled.\n","\n","--- Checking for Duplicates ---\n","Removing 1519 duplicate rows.\n","\n","--- Handling Outliers ---\n","\n","--- Final Dataset Overview ---\n","Shape after preprocessing: (210357, 5)\n","Sample Data:\n","       reviewer_id                                   purchase_history  \\\n","0  A1L3BAYQ7DZWZS  [{'item': {'asin': 'B014DUB312', 'title': \"Fau...   \n","1  A2DXOK3HLBPZYG  [{'item': {'asin': 'B00NWA56L8', 'title': 'Har...   \n","2  A2FFXP452K3J8E  [{'item': {'asin': 'B00B9FKH94', 'title': \"Emp...   \n","3  A1PPLJ50ZJN58T  [{'item': {'asin': 'B001V9LM4M', 'title': \"Ame...   \n","4   AQ4LZBULIOAQI  [{'item': {'asin': 'B0001YR5AI', 'title': 'Dic...   \n","\n","                                       last_purchase  \\\n","0  {'reason': 'The customer likely needed replace...   \n","1  {'reason': 'The customer likely needed a new s...   \n","2  {'reason': 'Needed comfortable sneakers for ca...   \n","3  {'reason': 'The customer wanted a comfortable,...   \n","4  {'reason': 'The customer likely purchased a be...   \n","\n","                                                user  \\\n","0  {'profile': 'Fashionable, Value-focused Shoppe...   \n","1  {'profile': 'Recreational, Comfortable, Casual...   \n","2  {'profile': 'Casual, Comfortable, Male, Shoppe...   \n","3  {'profile': 'Male, Stylish, Comfortable, Boot ...   \n","4  {'profile': 'Practical, functional, warm weath...   \n","\n","                                            metadata  \n","0  {'example': {'richness': 3, 'confidence': 5}, ...  \n","1  {'example': {'richness': 5, 'confidence': 7}, ...  \n","2  {'example': {'richness': 3, 'confidence': 6}, ...  \n","3  {'example': {'richness': 5, 'confidence': 5}, ...  \n","4  {'example': {'richness': 5, 'confidence': 5}, ...  \n","\n","Cleaned dataset saved to clothing_sampled_cleaned.csv.\n"]}],"source":["import pandas as pd\n","import numpy as np\n","\n","def load_and_preprocess(file_path):\n","    try:\n","        # Step 1: Load the dataset\n","        df = pd.read_csv(file_path)\n","        print(\"File successfully loaded.\")\n","\n","        # Step 2: Basic dataset information\n","        print(\"\\n--- Dataset Overview ---\")\n","        print(f\"Shape: {df.shape}\")\n","        print(\"Columns:\", df.columns.tolist())\n","        print(\"\\nData Types:\\n\", df.dtypes)\n","        print(\"\\nMissing Values:\\n\", df.isnull().sum())\n","\n","        # Step 3: Standardize column names\n","        df.columns = [col.strip().lower().replace(' ', '_') for col in df.columns]\n","        print(\"\\nStandardized Column Names:\", df.columns.tolist())\n","\n","        # Step 4: Handle missing values\n","        print(\"\\n--- Handling Missing Values ---\")\n","        missing_threshold = 0.3  # Drop columns with >30% missing data\n","        missing_cols = df.columns[df.isnull().mean() > missing_threshold]\n","        if len(missing_cols) > 0:\n","            print(f\"Dropping columns with >{missing_threshold*100}% missing values: {missing_cols.tolist()}\")\n","            df.drop(columns=missing_cols, inplace=True)\n","        else:\n","            print(\"No columns dropped for missing values.\")\n","\n","        # Fill remaining missing values\n","        df.fillna(method='ffill', inplace=True)  # Forward fill as default\n","        df.fillna(method='bfill', inplace=True)  # Backward fill for remaining\n","        print(\"Remaining missing values filled.\")\n","\n","        # Step 5: Remove duplicates\n","        print(\"\\n--- Checking for Duplicates ---\")\n","        duplicates = df.duplicated().sum()\n","        if duplicates > 0:\n","            print(f\"Removing {duplicates} duplicate rows.\")\n","            df = df.drop_duplicates()\n","        else:\n","            print(\"No duplicates found.\")\n","\n","        # Step 6: Detect and handle outliers (example for numerical columns)\n","        print(\"\\n--- Handling Outliers ---\")\n","        num_cols = df.select_dtypes(include=np.number).columns\n","        for col in num_cols:\n","            q1, q3 = df[col].quantile([0.25, 0.75])\n","            iqr = q3 - q1\n","            lower_bound = q1 - 1.5 * iqr\n","            upper_bound = q3 + 1.5 * iqr\n","            outliers = ((df[col] < lower_bound) | (df[col] > upper_bound)).sum()\n","            if outliers > 0:\n","                print(f\"Column '{col}' has {outliers} outliers. Clipping values.\")\n","                df[col] = np.clip(df[col], lower_bound, upper_bound)\n","\n","        # Step 7: Provide final dataset overview\n","        print(\"\\n--- Final Dataset Overview ---\")\n","        print(f\"Shape after preprocessing: {df.shape}\")\n","        print(\"Sample Data:\\n\", df.head())\n","\n","        # Step 8: Save cleaned data\n","        clean_file_path = file_path.replace('.csv', '_cleaned.csv')\n","        df.to_csv(clean_file_path, index=False)\n","        print(f\"\\nCleaned dataset saved to {clean_file_path}.\")\n","        return df\n","\n","    except FileNotFoundError:\n","        print(\"Error: File not found. Please ensure the file path is correct.\")\n","    except pd.errors.EmptyDataError:\n","        print(\"Error: File is empty.\")\n","    except pd.errors.ParserError:\n","        print(\"Error: Could not parse the file. Check file format.\")\n","    except Exception as e:\n","        print(f\"An unexpected error occurred: {e}\")\n","\n","# Run the function with your file path\n","df_cleaned = load_and_preprocess('clothing_sampled.csv')\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":44478,"status":"ok","timestamp":1733634529494,"user":{"displayName":"Julian Carbajal","userId":"13737318245966064601"},"user_tz":480},"id":"ffrFW06Vlzma","outputId":"f34a09b2-7ee0-480b-dd7e-054be8bee97f"},"outputs":[{"ename":"NameError","evalue":"name 'df' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[2], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m df_cleaned\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./clothing_sampled_cleaned.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Check columns in the dataset\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Select the column containing textual data for sentiment analysis\u001b[39;00m\n\u001b[0;32m      9\u001b[0m text_column \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview_text\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Replace with the actual column name in your dataset\u001b[39;00m\n","\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"]}],"source":["import pandas as pd\n","\n","# Save cleaned data for Shiny app\n","df_cleaned.to_csv('./clothing_sampled_cleaned.csv', index=False)\n","# Check columns in the dataset\n","print(df.columns)\n","\n","# Select the column containing textual data for sentiment analysis\n","text_column = 'review_text'  # Replace with the actual column name in your dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":116,"status":"ok","timestamp":1733634708568,"user":{"displayName":"Julian Carbajal","userId":"13737318245966064601"},"user_tz":480},"id":"hJR8_9Y1qtzz","outputId":"ab53b156-5467-48e7-e74a-5fb1703d6cad"},"outputs":[],"source":["import pandas as pd\n","import ast  # For safely evaluating stringified dictionaries\n","from textblob import TextBlob\n","\n","# Sample DataFrame\n","data = {\n","    \"reviewer_id\": [\n","        \"A1L3BAYQ7DZWZS\", \"A2DXOK3HLBPZYG\", \"A2FFXP452K3J8E\", \"A1PPLJ50ZJN58T\", \"AQ4LZBULIOAQI\"\n","    ],\n","    \"last_purchase\": [\n","        \"{'reason': 'The customer likely needed replacement parts for their device.'}\",\n","        \"{'reason': 'The customer likely needed a new style for summer.'}\",\n","        \"{'reason': 'Needed comfortable sneakers for casual wear.'}\",\n","        \"{'reason': 'The customer wanted a comfortable, stylish shoe for walking.'}\",\n","        \"{'reason': 'The customer likely purchased a beach hat for vacation.'}\"\n","    ]\n","}\n","\n","df = pd.DataFrame(data)\n","\n","# Extract the 'reason' field\n","def extract_reason(purchase):\n","    try:\n","        purchase_dict = ast.literal_eval(purchase)  # Convert string to dictionary\n","        return purchase_dict.get('reason', None)\n","    except:\n","        return None\n","\n","df['reason_text'] = df['last_purchase'].apply(extract_reason)\n","\n","# Display extracted reasons\n","print(df[['reviewer_id', 'reason_text']])"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":96,"status":"ok","timestamp":1733634803935,"user":{"displayName":"Julian Carbajal","userId":"13737318245966064601"},"user_tz":480},"id":"r28hFRwIr8Ay"},"outputs":[],"source":["# Define positive and negative keywords\n","positive_keywords = [\"comfortable\", \"stylish\", \"new\", \"great\", \"perfect\", \"love\", \"excellent\", \"vacation\", \"enjoy\"]\n","negative_keywords = [\"needed\", \"replacement\", \"issue\", \"problem\", \"broken\", \"poor\", \"dissatisfied\", \"complaint\"]\n","\n","# Define a function to calculate keyword-based sentiment\n","def keyword_sentiment(text):\n","    if pd.isnull(text):  # Handle missing values\n","        return None\n","    text = text.lower()  # Convert text to lowercase for comparison\n","    positive_count = sum(word in text for word in positive_keywords)\n","    negative_count = sum(word in text for word in negative_keywords)\n","    score = positive_count - negative_count  # Positive if more positive keywords, negative if more negative keywords\n","    return score"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":84,"status":"ok","timestamp":1733634825314,"user":{"displayName":"Julian Carbajal","userId":"13737318245966064601"},"user_tz":480},"id":"wps7hEAIsBRW"},"outputs":[],"source":["# Combine keyword-based and TextBlob sentiment\n","def hybrid_sentiment(text):\n","    if pd.isnull(text):  # Handle missing values\n","        return None\n","    # Keyword-based sentiment score\n","    keyword_score = keyword_sentiment(text)\n","    # TextBlob sentiment score\n","    blob_score = TextBlob(text).sentiment.polarity\n","    # Combine scores (you can tune weights if needed)\n","    combined_score = blob_score + (0.5 * keyword_score)  # Give more weight to TextBlob or keywords as needed\n","    return combined_score"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":104,"status":"ok","timestamp":1733634832458,"user":{"displayName":"Julian Carbajal","userId":"13737318245966064601"},"user_tz":480},"id":"JraKzGO0sCPN","outputId":"56bcd1e6-4b82-4dc9-df42-c6aef2c2696e"},"outputs":[],"source":["# Apply the hybrid sentiment scoring function\n","df['hybrid_sentiment_score'] = df['reason_text'].apply(hybrid_sentiment)\n","\n","# Classify sentiment based on the hybrid score\n","def classify_hybrid_sentiment(score):\n","    if score is None:\n","        return None\n","    elif score > 0:\n","        return 'positive'\n","    elif score < 0:\n","        return 'negative'\n","    else:\n","        return 'neutral'\n","\n","df['hybrid_sentiment_label'] = df['hybrid_sentiment_score'].apply(classify_hybrid_sentiment)\n","\n","# Display results\n","print(df[['reviewer_id', 'reason_text', 'hybrid_sentiment_score', 'hybrid_sentiment_label']])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":595,"status":"ok","timestamp":1733634859602,"user":{"displayName":"Julian Carbajal","userId":"13737318245966064601"},"user_tz":480},"id":"E9L9VCYisJiu","outputId":"f75926d4-4d4a-4e06-d06f-f788cb9a5860"},"outputs":[],"source":["import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# Countplot for sentiment labels\n","sns.countplot(data=df, x='hybrid_sentiment_label', palette='coolwarm')\n","plt.title(\"Distribution of Hybrid Sentiment Labels\")\n","plt.show()\n","\n","# Sentiment score distribution\n","sns.histplot(df['hybrid_sentiment_score'].dropna(), kde=True, color='purple')\n","plt.title(\"Hybrid Sentiment Score Distribution\")\n","plt.xlabel(\"Hybrid Sentiment Score\")\n","plt.ylabel(\"Frequency\")\n","plt.show()\n","\n","# Save results to a CSV file\n","df.to_csv('./clothing_sampled_with_hybrid_sentiment.csv', index=False)\n","print(\"Hybrid sentiment analysis results saved to clothing_sampled_with_hybrid_sentiment.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":84,"status":"ok","timestamp":1733634715435,"user":{"displayName":"Julian Carbajal","userId":"13737318245966064601"},"user_tz":480},"id":"q2FhNnzFnZd1","outputId":"38c09afe-6f1a-4dc7-e1e7-9f60318c8c37"},"outputs":[],"source":["# Define a function to calculate sentiment polarity\n","def get_sentiment(text):\n","    if pd.isnull(text):  # Handle missing values\n","        return None\n","    analysis = TextBlob(text)\n","    return analysis.sentiment.polarity\n","\n","# Apply sentiment analysis\n","df['sentiment_score'] = df['reason_text'].apply(get_sentiment)\n","\n","# Classify sentiment\n","def classify_sentiment(score):\n","    if score is None:\n","        return None\n","    elif score > 0:\n","        return 'positive'\n","    elif score < 0:\n","        return 'negative'\n","    else:\n","        return 'neutral'\n","\n","df['sentiment_label'] = df['sentiment_score'].apply(classify_sentiment)\n","\n","# Display results\n","print(df[['reviewer_id', 'reason_text', 'sentiment_score', 'sentiment_label']])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":123,"status":"ok","timestamp":1733634728908,"user":{"displayName":"Julian Carbajal","userId":"13737318245966064601"},"user_tz":480},"id":"pWWQUOx4nd5B","outputId":"e15695e6-4421-4e83-85f9-d58dbe9f8f82"},"outputs":[],"source":["df.to_csv('/content/clothing_sampled_with_sentiment.csv', index=False)\n","print(\"Sentiment analysis results saved to clothing_sampled_with_sentiment.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":1077,"status":"ok","timestamp":1733634736783,"user":{"displayName":"Julian Carbajal","userId":"13737318245966064601"},"user_tz":480},"id":"mB2gFN4hrrb9","outputId":"8c59dba0-42c6-490b-bffc-6efe2a4abc37"},"outputs":[],"source":["import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# Countplot for sentiment labels\n","sns.countplot(data=df, x='sentiment_label', palette='coolwarm')\n","plt.title(\"Distribution of Sentiment Labels\")\n","plt.show()\n","\n","# Sentiment score distribution\n","sns.histplot(df['sentiment_score'].dropna(), kde=True, color='blue')\n","plt.title(\"Sentiment Score Distribution\")\n","plt.xlabel(\"Sentiment Score\")\n","plt.ylabel(\"Frequency\")\n","plt.show()"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":244007,"status":"ok","timestamp":1733635248687,"user":{"displayName":"Julian Carbajal","userId":"13737318245966064601"},"user_tz":480},"id":"skX6POVostBA","outputId":"ac7dfc15-1fd9-4c7c-d204-ac034ed37a1d"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Sample Data with Sentiment Labels:\n","                                         reason_text sentiment_label\n","0  The customer likely needed replacement lenses ...         neutral\n","1  The customer likely needed a new swimsuit for ...        positive\n","2  Needed comfortable sneakers for casual wear bu...        negative\n","3  The customer wanted a comfortable, high-qualit...        negative\n","4  The customer likely purchased a beanie as a gi...         neutral\n","\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","    negative       0.90      0.79      0.84      6708\n","     neutral       0.84      0.93      0.88      8914\n","    positive       0.95      0.94      0.95     26450\n","\n","    accuracy                           0.92     42072\n","   macro avg       0.90      0.89      0.89     42072\n","weighted avg       0.92      0.92      0.92     42072\n","\n","\n","Accuracy: 0.9164765164479939\n","\n","Results saved to ./clothing_sampled_with_sentiment.csv\n"]}],"source":["# Import necessary libraries\n","import pandas as pd\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report, accuracy_score\n","from textblob import TextBlob\n","import ast\n","\n","# Step 1: Load the cleaned dataset\n","file_path = './clothing_sampled_cleaned.csv'  # Adjust the path if necessary\n","df = pd.read_csv(file_path)\n","\n","# Step 2: Extract 'reason_text' from the 'last_purchase' column\n","def extract_reason(purchase):\n","    try:\n","        purchase_dict = ast.literal_eval(purchase)  # Convert string to dictionary\n","        return purchase_dict.get('reason', None)\n","    except:\n","        return None\n","\n","df['reason_text'] = df['last_purchase'].apply(extract_reason)\n","\n","# Step 3: Generate sentiment labels using TextBlob\n","def get_sentiment(text):\n","    if pd.isnull(text):  # Handle missing values\n","        return None\n","    analysis = TextBlob(text)\n","    polarity = analysis.sentiment.polarity\n","    if polarity > 0:\n","        return 'positive'\n","    elif polarity < 0:\n","        return 'negative'\n","    else:\n","        return 'neutral'\n","\n","df['sentiment_label'] = df['reason_text'].apply(get_sentiment)\n","\n","# Check if sentiment labels were generated\n","print(\"\\nSample Data with Sentiment Labels:\")\n","print(df[['reason_text', 'sentiment_label']].head())\n","\n","# Step 4: Text vectorization using TF-IDF\n","vectorizer = TfidfVectorizer(max_features=500)\n","X_text = vectorizer.fit_transform(df['reason_text'].fillna('')).toarray()\n","\n","# Step 5: Prepare data for classification\n","X = X_text  # Features (add more if needed)\n","y = df['sentiment_label']  # Target variable\n","\n","# Split into train and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Step 6: Train a classification model\n","clf = RandomForestClassifier(random_state=42)\n","clf.fit(X_train, y_train)\n","\n","# Step 7: Make predictions and evaluate the model\n","y_pred = clf.predict(X_test)\n","\n","# Print evaluation metrics\n","print(\"\\nClassification Report:\")\n","print(classification_report(y_test, y_pred))\n","print(\"\\nAccuracy:\", accuracy_score(y_test, y_pred))\n","\n","# Save the updated DataFrame with sentiment labels\n","output_file = './clothing_sampled_with_sentiment.csv'\n","df.to_csv(output_file, index=False)\n","print(f\"\\nResults saved to {output_file}\")"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>reviewer_id</th>\n","      <th>purchase_history</th>\n","      <th>last_purchase</th>\n","      <th>user</th>\n","      <th>metadata</th>\n","      <th>reason_text</th>\n","      <th>sentiment_label</th>\n","      <th>sentiment_numeric</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>A1L3BAYQ7DZWZS</td>\n","      <td>[{'item': {'asin': 'B014DUB312', 'title': \"Fau...</td>\n","      <td>{'reason': 'The customer likely needed replace...</td>\n","      <td>{'profile': 'Fashionable, Value-focused Shoppe...</td>\n","      <td>{'example': {'richness': 3, 'confidence': 5}, ...</td>\n","      <td>The customer likely needed replacement lenses ...</td>\n","      <td>neutral</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>A2DXOK3HLBPZYG</td>\n","      <td>[{'item': {'asin': 'B00NWA56L8', 'title': 'Har...</td>\n","      <td>{'reason': 'The customer likely needed a new s...</td>\n","      <td>{'profile': 'Recreational, Comfortable, Casual...</td>\n","      <td>{'example': {'richness': 5, 'confidence': 7}, ...</td>\n","      <td>The customer likely needed a new swimsuit for ...</td>\n","      <td>positive</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>A2FFXP452K3J8E</td>\n","      <td>[{'item': {'asin': 'B00B9FKH94', 'title': \"Emp...</td>\n","      <td>{'reason': 'Needed comfortable sneakers for ca...</td>\n","      <td>{'profile': 'Casual, Comfortable, Male, Shoppe...</td>\n","      <td>{'example': {'richness': 3, 'confidence': 6}, ...</td>\n","      <td>Needed comfortable sneakers for casual wear bu...</td>\n","      <td>negative</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>A1PPLJ50ZJN58T</td>\n","      <td>[{'item': {'asin': 'B001V9LM4M', 'title': \"Ame...</td>\n","      <td>{'reason': 'The customer wanted a comfortable,...</td>\n","      <td>{'profile': 'Male, Stylish, Comfortable, Boot ...</td>\n","      <td>{'example': {'richness': 5, 'confidence': 5}, ...</td>\n","      <td>The customer wanted a comfortable, high-qualit...</td>\n","      <td>negative</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>AQ4LZBULIOAQI</td>\n","      <td>[{'item': {'asin': 'B0001YR5AI', 'title': 'Dic...</td>\n","      <td>{'reason': 'The customer likely purchased a be...</td>\n","      <td>{'profile': 'Practical, functional, warm weath...</td>\n","      <td>{'example': {'richness': 5, 'confidence': 5}, ...</td>\n","      <td>The customer likely purchased a beanie as a gi...</td>\n","      <td>neutral</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      reviewer_id                                   purchase_history  \\\n","0  A1L3BAYQ7DZWZS  [{'item': {'asin': 'B014DUB312', 'title': \"Fau...   \n","1  A2DXOK3HLBPZYG  [{'item': {'asin': 'B00NWA56L8', 'title': 'Har...   \n","2  A2FFXP452K3J8E  [{'item': {'asin': 'B00B9FKH94', 'title': \"Emp...   \n","3  A1PPLJ50ZJN58T  [{'item': {'asin': 'B001V9LM4M', 'title': \"Ame...   \n","4   AQ4LZBULIOAQI  [{'item': {'asin': 'B0001YR5AI', 'title': 'Dic...   \n","\n","                                       last_purchase  \\\n","0  {'reason': 'The customer likely needed replace...   \n","1  {'reason': 'The customer likely needed a new s...   \n","2  {'reason': 'Needed comfortable sneakers for ca...   \n","3  {'reason': 'The customer wanted a comfortable,...   \n","4  {'reason': 'The customer likely purchased a be...   \n","\n","                                                user  \\\n","0  {'profile': 'Fashionable, Value-focused Shoppe...   \n","1  {'profile': 'Recreational, Comfortable, Casual...   \n","2  {'profile': 'Casual, Comfortable, Male, Shoppe...   \n","3  {'profile': 'Male, Stylish, Comfortable, Boot ...   \n","4  {'profile': 'Practical, functional, warm weath...   \n","\n","                                            metadata  \\\n","0  {'example': {'richness': 3, 'confidence': 5}, ...   \n","1  {'example': {'richness': 5, 'confidence': 7}, ...   \n","2  {'example': {'richness': 3, 'confidence': 6}, ...   \n","3  {'example': {'richness': 5, 'confidence': 5}, ...   \n","4  {'example': {'richness': 5, 'confidence': 5}, ...   \n","\n","                                         reason_text sentiment_label  \\\n","0  The customer likely needed replacement lenses ...         neutral   \n","1  The customer likely needed a new swimsuit for ...        positive   \n","2  Needed comfortable sneakers for casual wear bu...        negative   \n","3  The customer wanted a comfortable, high-qualit...        negative   \n","4  The customer likely purchased a beanie as a gi...         neutral   \n","\n","   sentiment_numeric  \n","0                  1  \n","1                  2  \n","2                  0  \n","3                  0  \n","4                  1  "]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["df.head()"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"text/plain":["['sentiment_classification.joblib']"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["#save file\n","import joblib\n","\n","# Save the trained model using joblib\n","joblib.dump(clf, 'sentiment_classification.joblib')"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":195542,"status":"ok","timestamp":1733635778812,"user":{"displayName":"Julian Carbajal","userId":"13737318245966064601"},"user_tz":480},"id":"0-9jpZDBu4uo","outputId":"60415732-143e-4f96-b09b-2be3021facdc"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Sample Data with Sentiment Labels:\n","                                         reason_text sentiment_label\n","0  The customer likely needed replacement lenses ...         neutral\n","1  The customer likely needed a new swimsuit for ...        positive\n","2  Needed comfortable sneakers for casual wear bu...        negative\n","3  The customer wanted a comfortable, high-qualit...        negative\n","4  The customer likely purchased a beanie as a gi...         neutral\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\mfave\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [13:59:52] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  warnings.warn(smsg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","    negative       0.92      0.84      0.88      6708\n","     neutral       0.83      0.98      0.90      8914\n","    positive       0.98      0.94      0.96     26450\n","\n","    accuracy                           0.93     42072\n","   macro avg       0.91      0.92      0.91     42072\n","weighted avg       0.94      0.93      0.93     42072\n","\n","\n","Accuracy: 0.9326155162578437\n","\n","Results saved to ./clothing_sampled_with_xgboost_sentiment.csv\n"]}],"source":["#XGBoost\n","\n","# Import necessary libraries\n","import pandas as pd\n","import ast\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report, accuracy_score\n","from textblob import TextBlob\n","import xgboost as xgb\n","\n","# Step 1: Load the cleaned dataset\n","file_path = './clothing_sampled_cleaned.csv'  # Adjust the path if necessary\n","df = pd.read_csv(file_path)\n","\n","# Step 2: Extract 'reason_text' from the 'last_purchase' column\n","def extract_reason(purchase):\n","    try:\n","        purchase_dict = ast.literal_eval(purchase)  # Convert string to dictionary\n","        return purchase_dict.get('reason', None)\n","    except:\n","        return None\n","\n","df['reason_text'] = df['last_purchase'].apply(extract_reason)\n","\n","# Step 3: Generate sentiment labels using TextBlob\n","def get_sentiment(text):\n","    if pd.isnull(text):  # Handle missing values\n","        return None\n","    analysis = TextBlob(text)\n","    polarity = analysis.sentiment.polarity\n","    if polarity > 0:\n","        return 'positive'\n","    elif polarity < 0:\n","        return 'negative'\n","    else:\n","        return 'neutral'\n","\n","df['sentiment_label'] = df['reason_text'].apply(get_sentiment)\n","\n","# Check if sentiment labels were generated\n","print(\"\\nSample Data with Sentiment Labels:\")\n","print(df[['reason_text', 'sentiment_label']].head())\n","\n","# Step 4: Text vectorization using TF-IDF\n","vectorizer = TfidfVectorizer(max_features=500)\n","X_text = vectorizer.fit_transform(df['reason_text'].fillna('')).toarray()\n","\n","# Encode target labels to numeric format for XGBoost\n","label_mapping = {'positive': 2, 'neutral': 1, 'negative': 0}\n","df['sentiment_numeric'] = df['sentiment_label'].map(label_mapping)\n","\n","# Step 5: Prepare data for classification\n","X = X_text  # Features (add more if needed)\n","y = df['sentiment_numeric']  # Numeric target variable\n","\n","# Split into train and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Step 6: Train an XGBoost classification model\n","xgb_model = xgb.XGBClassifier(\n","    objective='multi:softmax',  # Multiclass classification\n","    num_class=3,  # Three classes: positive, neutral, negative\n","    eval_metric='mlogloss',  # Log loss for multiclass\n","    use_label_encoder=False,\n","    random_state=42\n",")\n","xgb_model.fit(X_train, y_train)\n","\n","# Step 7: Make predictions and evaluate the model\n","y_pred = xgb_model.predict(X_test)\n","\n","# Convert numeric predictions back to labels for interpretation\n","reverse_label_mapping = {v: k for k, v in label_mapping.items()}\n","y_test_labels = y_test.map(reverse_label_mapping)\n","y_pred_labels = pd.Series(y_pred).map(reverse_label_mapping)\n","\n","# Print evaluation metrics\n","print(\"\\nClassification Report:\")\n","print(classification_report(y_test_labels, y_pred_labels))\n","print(\"\\nAccuracy:\", accuracy_score(y_test_labels, y_pred_labels))\n","\n","# Save the updated DataFrame with sentiment labels\n","output_file = './clothing_sampled_with_xgboost_sentiment.csv'\n","df.to_csv(output_file, index=False)\n","print(f\"\\nResults saved to {output_file}\")"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1733635579373,"user":{"displayName":"Julian Carbajal","userId":"13737318245966064601"},"user_tz":480},"id":"ua6NfeqZu5y4"},"outputs":[{"data":{"text/plain":["['xgboost_sentiment_classification.joblib']"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["import joblib\n","\n","# Save the trained model using joblib\n","joblib.dump(xgb_model, 'xgboost_sentiment_classification.joblib')"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMlNZDlKs1ustf4Msip4J+S","gpuType":"A100","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":0}
